{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TCN multi-output pipeline (poll_lens & poll_pw)\n",
        "- DOWNSAMPLE = 20, SEQ_LEN = 300, HORIZON = 1 hour\n",
        "- Clip targets to [0,100], robust global scalers, NaN/Inf sanitization\n",
        "- Vectorized windows via sliding_window_view\n",
        "- K-Fold (5) + final forced test on files 23 & 25\n",
        "- Designed for Google Colab A100 (num_workers=0, AMP)\n",
        "\"\"\"\n",
        "import os, gc, time, math, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "\n",
        "# sklearn\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n"
      ],
      "metadata": {
        "id": "7Wsiz_RL-wPv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# USER SETTINGS\n",
        "# -----------------------\n",
        "CSV_PATH = \"/content/drive/MyDrive/Dataset_csv_merged_v2\"\n",
        "N_FILES = 35\n",
        "EXCLUDE_FILES = {8, 19, 33}\n",
        "MANDATORY_TEST = {23, 25}\n",
        "DOWNSAMPLE = 20\n",
        "RAW_STEP_SECONDS = 0.05            # 50 ms\n",
        "RAW_STEPS_PER_HOUR = int(3600 / RAW_STEP_SECONDS)  # 72 000\n",
        "STEPS_PER_HOUR = RAW_STEPS_PER_HOUR // DOWNSAMPLE   # 3 600 when DOWNSAMPLE=20\n",
        "\n",
        "SEQ_LEN = 300                      # context window\n",
        "HORIZON = STEPS_PER_HOUR           # prediction horizon -> 1 hour\n",
        "K_FOLDS = 5\n",
        "\n",
        "MAX_ROWS_PER_FILE = 200_000\n",
        "MAX_SEQS_PER_FILE = 20_000         # cap windows per file\n",
        "SAMPLE_ROWS_FOR_SCALER = 120_000   # rows sampled across files to fit scalers\n",
        "MAX_SAMPLES_PER_FILE_FOR_SCALER = 12_000\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 12\n",
        "LR = 8e-4\n",
        "PATIENCE = 4\n",
        "\n",
        "CACHE_DIR = \"/content/sequence_cache_final\"\n",
        "MODEL_DIR = \"/content/tcn_models_final\"\n",
        "RESULTS_FILE = \"/content/tcn_results_final.pkl\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "FEATURE_COLS_BASE = [\n",
        "    'Time', 'Temperature lens IR', 'Temperature window IR', 'Temperature window BTS',\n",
        "    'Pressure', 'iPartOpType', 'AC_VACTW', 'Laser set power', 'Gas set pressure',\n",
        "    'Gas type', 'Mach code', 'Bar material', 'Bar thickness', 'csv_time_sec'\n",
        "]\n",
        "TARGETS = ['poll_lens', 'poll_pw']\n",
        "CATEGORICAL_COLS = ['Gas type', 'Mach code', 'Bar material', 'iPartOpType']\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "print(f\"SEQ_LEN={SEQ_LEN}, DOWNSAMPLE={DOWNSAMPLE}, HORIZON={HORIZON}, STEPS_PER_HOUR={STEPS_PER_HOUR}\")\n"
      ],
      "metadata": {
        "id": "F5cK7ok38B2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# UTIL: file mapping (1-based indices)\n",
        "# -----------------------\n",
        "def get_file_map(csv_folder):\n",
        "    files = sorted([f for f in os.listdir(csv_folder) if f.endswith('.csv')])\n",
        "    mapping = {}\n",
        "    idx = 1\n",
        "    for fname in files:\n",
        "        mapping[idx] = os.path.join(csv_folder, fname)\n",
        "        idx += 1\n",
        "    return mapping\n",
        "\n",
        "file_map = get_file_map(CSV_PATH)\n",
        "usable_files = [i for i in range(1, N_FILES+1) if i in file_map and i not in EXCLUDE_FILES]\n",
        "print(\"Usable files (1-based):\", usable_files)\n",
        "\n",
        "# -----------------------\n",
        "# CATEGORY LEVEL COLLECTION (stable one-hot)\n",
        "# -----------------------\n",
        "def collect_category_levels(file_indices, max_rows_each=3000):\n",
        "    levels = {c: set() for c in CATEGORICAL_COLS}\n",
        "    for idx in file_indices:\n",
        "        path = file_map.get(idx)\n",
        "        if path is None: continue\n",
        "        try:\n",
        "            # read only few rows to collect categories\n",
        "            df = pd.read_csv(path, nrows=max_rows_each, usecols=[c for c in CATEGORICAL_COLS if c in pd.read_csv(path, nrows=0).columns])\n",
        "        except Exception:\n",
        "            continue\n",
        "        for c in CATEGORICAL_COLS:\n",
        "            if c in df.columns:\n",
        "                for v in df[c].dropna().astype(str).unique().tolist():\n",
        "                    levels[c].add(v)\n",
        "        del df; gc.collect()\n",
        "    for c in levels:\n",
        "        levels[c] = sorted(list(levels[c]))[:200]\n",
        "    return levels\n",
        "\n",
        "# -----------------------\n",
        "# FEATURIZE single raw df (pandas) -> returns cleaned numeric df with targets at end\n",
        "# includes clipping of targets to [0,100] and global NaN/Inf handling\n",
        "# -----------------------\n",
        "def featurize_df(raw_df, categorical_levels):\n",
        "    df = raw_df.copy()\n",
        "    # ensure targets exist\n",
        "    if 'poll_lens' not in df.columns or 'poll_pw' not in df.columns:\n",
        "        return None\n",
        "    # Clip targets to [0,100]\n",
        "    df['poll_lens'] = pd.to_numeric(df['poll_lens'], errors='coerce').clip(0,100)\n",
        "    df['poll_pw']   = pd.to_numeric(df['poll_pw'], errors='coerce').clip(0,100)\n",
        "\n",
        "    # Replace inf and fill NaN (forward/backfill then zeros)\n",
        "    df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill().fillna(0)\n",
        "\n",
        "    # Time cyclical\n",
        "    if 'Time' in df.columns:\n",
        "        day_ms = 24*3600*1000\n",
        "        tnorm = (df['Time'] % day_ms) / day_ms\n",
        "        df['time_sin'] = np.sin(2*np.pi*tnorm)\n",
        "        df['time_cos'] = np.cos(2*np.pi*tnorm)\n",
        "        df['hour'] = (df['Time'] % day_ms) / (3600*1000)\n",
        "\n",
        "    # Temperature lens features\n",
        "    if 'Temperature lens IR' in df.columns:\n",
        "        t = pd.to_numeric(df['Temperature lens IR'], errors='coerce').fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "        df['temp_lens_ma5'] = t.rolling(5, min_periods=1).mean()\n",
        "        df['temp_lens_trend'] = t.diff().fillna(0)\n",
        "\n",
        "    # Pressure features\n",
        "    if 'Pressure' in df.columns:\n",
        "        p = pd.to_numeric(df['Pressure'], errors='coerce').fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "        df['press_ma10'] = p.rolling(10, min_periods=1).mean()\n",
        "        df['press_diff'] = p.diff().fillna(0)\n",
        "\n",
        "    # Window diffs\n",
        "    if 'Temperature window IR' in df.columns and 'Temperature window BTS' in df.columns:\n",
        "        df['temp_window_diff'] = pd.to_numeric(df['Temperature window IR'], errors='coerce').fillna(0) - pd.to_numeric(df['Temperature window BTS'], errors='coerce').fillna(0)\n",
        "        df['lens_window_diff'] = pd.to_numeric(df['Temperature lens IR'], errors='coerce').fillna(0) - pd.to_numeric(df['Temperature window IR'], errors='coerce').fillna(0)\n",
        "\n",
        "    # Poll rollings\n",
        "    df['poll_lens_ma7'] = pd.to_numeric(df['poll_lens'], errors='coerce').rolling(7, min_periods=1).mean().fillna(0)\n",
        "    df['poll_lens_trend'] = pd.to_numeric(df['poll_lens'], errors='coerce').diff().fillna(0)\n",
        "    df['poll_pw_ma7'] = pd.to_numeric(df['poll_pw'], errors='coerce').rolling(7, min_periods=1).mean().fillna(0)\n",
        "    df['poll_pw_trend'] = pd.to_numeric(df['poll_pw'], errors='coerce').diff().fillna(0)\n",
        "\n",
        "    # Categorical one-hot stable\n",
        "    for c in CATEGORICAL_COLS:\n",
        "        levels = categorical_levels.get(c, [])\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].astype(str).fillna('NA')\n",
        "        else:\n",
        "            df[c] = 'NA'\n",
        "        for lvl in levels:\n",
        "            df[f\"{c}__{lvl}\"] = (df[c] == lvl).astype(float)\n",
        "        df[f\"{c}__other\"] = (~df[c].isin(levels)).astype(float)\n",
        "\n",
        "    # Drop originals cat columns and Time\n",
        "    df = df.drop(columns=[c for c in CATEGORICAL_COLS if c in df.columns], errors='ignore')\n",
        "    # Place targets at the end\n",
        "    target_lens = df['poll_lens'].astype(float).copy()\n",
        "    target_pw = df['poll_pw'].astype(float).copy()\n",
        "    df = df.drop(columns=['Time', 'poll_lens', 'poll_pw'], errors='ignore')\n",
        "    df['poll_lens'] = target_lens\n",
        "    df['poll_pw'] = target_pw\n",
        "    # Final sanitize\n",
        "    df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill().fillna(0)\n",
        "    return df\n",
        "\n",
        "# -----------------------\n",
        "# Vectorized window creation: takes scaled array (features + scaled targets last 2 cols),\n",
        "# returns windows X shape (N_windows, SEQ_LEN, n_features) and targets y shape (N_windows, 2)\n",
        "# Uses sliding_window_view, with uniform sampling if > max_seqs\n",
        "# -----------------------\n",
        "def create_windows_from_scaled_array(arr_scaled, seq_len, horizon, max_seqs_per_file):\n",
        "    n = arr_scaled.shape[0]\n",
        "    total_starts = n - seq_len - horizon + 1\n",
        "    if total_starts <= 0:\n",
        "        return None, None\n",
        "    # features portion excluding targets\n",
        "    arr_feat = arr_scaled[:-horizon, :-2]  # (n-horizon, n_feat)\n",
        "    # create windows\n",
        "    try:\n",
        "        windows = sliding_window_view(arr_feat, window_shape=(seq_len), axis=0)\n",
        "        # windows shape may be (total_starts, seq_len, n_feat)\n",
        "        windows = windows.reshape(windows.shape[0], windows.shape[1], windows.shape[2])\n",
        "    except Exception as e:\n",
        "        # fallback slower path (shouldn't happen)\n",
        "        out_w = []\n",
        "        for s in range(total_starts):\n",
        "            out_w.append(arr_feat[s:s+seq_len])\n",
        "        windows = np.stack(out_w, axis=0)\n",
        "    # targets indices\n",
        "    target_start_idx = seq_len + horizon - 1\n",
        "    targets = arr_scaled[target_start_idx: target_start_idx + windows.shape[0], -2:]  # (total_starts, 2)\n",
        "    total = windows.shape[0]\n",
        "    if total > max_seqs_per_file:\n",
        "        idxs = np.linspace(0, total-1, max_seqs_per_file, dtype=np.int32)\n",
        "        windows = windows[idxs]\n",
        "        targets = targets[idxs]\n",
        "    # final check: drop any window/target pair with NaN or Inf\n",
        "    mask_valid = (~np.isnan(windows).any(axis=(1,2))) & (~np.isnan(targets).any(axis=1)) & np.isfinite(windows).all(axis=(1,2)) & np.isfinite(targets).all(axis=1)\n",
        "    if not np.all(mask_valid):\n",
        "        windows = windows[mask_valid]\n",
        "        targets = targets[mask_valid]\n",
        "    if windows.shape[0] == 0:\n",
        "        return None, None\n",
        "    return windows.astype('float32'), targets.astype('float32')"
      ],
      "metadata": {
        "id": "AkHKv1sU8oJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Small TCN model (PyTorch)\n",
        "# -----------------------\n",
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super().__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size, dilation, padding, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size, padding=padding, dilation=dilation)\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size, padding=padding, dilation=dilation)\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
        "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
        "        self.downsample = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else None\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "class TCN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_channels=[32,32], kernel_size=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for i, out_ch in enumerate(num_channels):\n",
        "            in_ch = input_size if i == 0 else num_channels[i-1]\n",
        "            dilation = 2 ** i\n",
        "            padding = (kernel_size-1) * dilation\n",
        "            layers.append(TemporalBlock(in_ch, out_ch, kernel_size, dilation, padding, dropout))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "        self.fc = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Linear(num_channels[-1], 64), nn.ReLU(), nn.Dropout(0.2), nn.Linear(64, output_size))\n",
        "    def forward(self, x):\n",
        "        # x: (B, seq_len, features) -> (B, features, seq_len)\n",
        "        x = x.permute(0,2,1)\n",
        "        y = self.network(x)\n",
        "        out = self.fc(y)\n",
        "        return out"
      ],
      "metadata": {
        "id": "lY_vwZsD8-6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Metrics (unscaled)\n",
        "# -----------------------\n",
        "def compute_metrics(true, pred):\n",
        "    mask = true > 0.5\n",
        "    if np.sum(mask) == 0:\n",
        "        return None\n",
        "    t = true[mask]; p = pred[mask]\n",
        "    mae = float(np.mean(np.abs(t - p)))\n",
        "    rmse = float(np.sqrt(np.mean((t - p)**2)))\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        mape = float(np.mean(np.abs((t - p) / t)) * 100)\n",
        "        err_pct = np.abs((t - p) / t) * 100\n",
        "    r2 = float(1 - (np.sum((t - p)**2) / (np.sum((t - np.mean(t))**2) + 1e-12)))\n",
        "    pct5 = float(np.mean(err_pct < 5) * 100)\n",
        "    pct10 = float(np.mean(err_pct < 10) * 100)\n",
        "    pct15 = float(np.mean(err_pct < 15) * 100)\n",
        "    percentiles = np.percentile(err_pct, [25,50,75,90,95]).tolist()\n",
        "    return {'mae': mae, 'rmse': rmse, 'mape': mape, 'r2': r2, 'pct5': pct5, 'pct10': pct10, 'pct15': pct15, 'percentiles': percentiles}\n",
        "\n",
        "# -----------------------\n",
        "# PREP: build global scalers using samples across all usable files\n",
        "# -----------------------\n",
        "print(\"Fitting global scalers from sample rows across usable files...\")\n",
        "usable = usable_files\n",
        "cat_levels_global = collect_category_levels(usable, max_rows_each=3000)\n",
        "\n",
        "X_samples = []; y_samples = []; rows = 0\n",
        "for idx in usable:\n",
        "    p = file_map.get(idx);\n",
        "    if p is None: continue\n",
        "    try:\n",
        "        raw = pd.read_csv(p, usecols=lambda c: c in FEATURE_COLS_BASE + TARGETS, nrows=MAX_SAMPLES_PER_FILE_FOR_SCALER)\n",
        "    except Exception:\n",
        "        continue\n",
        "    if DOWNSAMPLE > 1:\n",
        "        raw = raw.iloc[::DOWNSAMPLE].reset_index(drop=True)\n",
        "    if len(raw) < 10:\n",
        "        continue\n",
        "    fdf = featurize_df(raw, cat_levels_global)\n",
        "    if fdf is None:\n",
        "        continue\n",
        "    vals = fdf.values\n",
        "    # append entire small sample\n",
        "    X_samples.append(vals[:, :-2])\n",
        "    y_samples.append(vals[:, -2:])\n",
        "    del raw, fdf, vals; gc.collect()\n",
        "    rows += 1\n",
        "    if rows >= len(usable):\n",
        "        break\n",
        "\n",
        "if len(X_samples) == 0:\n",
        "    raise RuntimeError(\"No sample rows to fit scalers! Check CSVs and columns.\")\n",
        "\n",
        "X_sample = np.vstack(X_samples)\n",
        "y_sample = np.vstack(y_samples)\n",
        "print(\"  Scaler sample shapes:\", X_sample.shape, y_sample.shape)\n",
        "\n",
        "# Use RobustScaler for X, StandardScaler for y (targets). StandardScaler ok since targets clipped [0,100]\n",
        "scaler_X_global = RobustScaler().fit(X_sample)\n",
        "scaler_y_global = StandardScaler().fit(y_sample)\n",
        "\n",
        "# free memory\n",
        "del X_samples, y_samples, X_sample, y_sample\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "0kBqPCqR9dRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# K-Fold training: vectorized windows per file (fast), training on stacked windows\n",
        "# -----------------------\n",
        "kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "all_fold_results = []\n",
        "global_true_lens = []; global_pred_lens = []\n",
        "global_true_pw = []; global_pred_pw = []\n",
        "\n",
        "fold_no = 0\n",
        "for train_idx, test_idx in kf.split(usable):\n",
        "    fold_no += 1\n",
        "    train_files = [usable[i] for i in train_idx]\n",
        "    test_files = [usable[i] for i in test_idx]\n",
        "    print(f\"\\n=== Fold {fold_no}/{K_FOLDS} â€” train {len(train_files)} files, test {len(test_files)} files ===\")\n",
        "    # collect cat levels for this fold (stability)\n",
        "    cat_levels = collect_category_levels(train_files, max_rows_each=2500)\n",
        "    # build train windows (vectorized) across files\n",
        "    X_parts = []; y_parts = []\n",
        "    for idx in train_files:\n",
        "        p = file_map.get(idx)\n",
        "        if p is None: continue\n",
        "        try:\n",
        "            raw = pd.read_csv(p, usecols=lambda c: c in FEATURE_COLS_BASE + TARGETS)\n",
        "        except Exception:\n",
        "            continue\n",
        "        if DOWNSAMPLE > 1:\n",
        "            raw = raw.iloc[::DOWNSAMPLE].reset_index(drop=True)\n",
        "        if len(raw) < SEQ_LEN + HORIZON:\n",
        "            continue\n",
        "        fdf = featurize_df(raw, cat_levels)\n",
        "        if fdf is None: continue\n",
        "        arr = fdf.values.astype('float32')\n",
        "        # scale\n",
        "        X_feats = scaler_X_global.transform(arr[:, :-2])\n",
        "        y_t = scaler_y_global.transform(arr[:, -2:])\n",
        "        arr_scaled = np.hstack([X_feats, y_t]).astype('float32')\n",
        "        Xw, yw = create_windows_from_scaled_array(arr_scaled, SEQ_LEN, HORIZON, MAX_SEQS_PER_FILE)\n",
        "        if Xw is None: continue\n",
        "        X_parts.append(Xw)\n",
        "        y_parts.append(yw)\n",
        "        del raw, fdf, arr, arr_scaled, X_feats, y_t, Xw, yw\n",
        "        gc.collect()\n",
        "    if len(X_parts) == 0:\n",
        "        print(\"No windows for train in this fold -> skip\")\n",
        "        continue\n",
        "    X_all = np.vstack(X_parts)\n",
        "    y_all = np.vstack(y_parts)\n",
        "    # shuffle and split\n",
        "    perm = np.random.permutation(len(X_all))\n",
        "    X_all = X_all[perm]; y_all = y_all[perm]\n",
        "    split = int(0.90 * len(X_all))\n",
        "    X_tr = X_all[:split]; y_tr = y_all[:split]\n",
        "    X_val = X_all[split:]; y_val = y_all[split:]\n",
        "    print(\"Train windows:\", X_tr.shape, \"Val windows:\", X_val.shape)\n",
        "    # free some RAM\n",
        "    del X_all, X_parts, y_all, y_parts; gc.collect()\n",
        "\n",
        "    # dataloaders (TensorDataset)\n",
        "    ds_tr = TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr))\n",
        "    ds_val = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "    dl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "    # model\n",
        "    n_features = X_tr.shape[2]\n",
        "    model = TCN(input_size=n_features, output_size=2, num_channels=[32,32], kernel_size=3, dropout=0.2).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "    amp = GradScaler()\n",
        "\n",
        "    best_val = 1e9; best_state = None; wait = 0\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        model.train()\n",
        "        train_losses = []; train_maes = []\n",
        "        for Xb, yb in dl_tr:\n",
        "            # sanitize & move to GPU\n",
        "            Xb = torch.nan_to_num(Xb, nan=0.0, posinf=0.0, neginf=0.0).to(DEVICE, dtype=torch.float)\n",
        "            yb = torch.nan_to_num(yb, nan=0.0, posinf=0.0, neginf=0.0).to(DEVICE, dtype=torch.float)\n",
        "            # skip if any invalid\n",
        "            if torch.isnan(Xb).any() or torch.isnan(yb).any():\n",
        "                continue\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                out = model(Xb)\n",
        "                loss = torch.nn.SmoothL1Loss()(out, yb)\n",
        "            amp.scale(loss).backward()\n",
        "            amp.step(optimizer)\n",
        "            amp.update()\n",
        "            train_losses.append(loss.item())\n",
        "            train_maes.append(torch.mean(torch.abs(out - yb)).item())\n",
        "        # validation\n",
        "        model.eval()\n",
        "        val_losses = []; val_maes = []\n",
        "        with torch.no_grad():\n",
        "            for Xb, yb in dl_val:\n",
        "                Xb = torch.nan_to_num(Xb, nan=0.0, posinf=0.0, neginf=0.0).to(DEVICE, dtype=torch.float)\n",
        "                yb = torch.nan_to_num(yb, nan=0.0, posinf=0.0, neginf=0.0).to(DEVICE, dtype=torch.float)\n",
        "                if torch.isnan(Xb).any() or torch.isnan(yb).any():\n",
        "                    continue\n",
        "                with autocast():\n",
        "                    out = model(Xb)\n",
        "                    loss = torch.nn.SmoothL1Loss()(out, yb)\n",
        "                val_losses.append(loss.item()); val_maes.append(torch.mean(torch.abs(out - yb)).item())\n",
        "        avg_val_mae = float(np.mean(val_maes)) if len(val_maes)>0 else float('inf')\n",
        "        val_loss_mean = float(np.mean(val_losses)) if len(val_losses)>0 else (np.mean(train_losses) if len(train_losses)>0 else 0.0)\n",
        "        scheduler.step(val_loss_mean)\n",
        "        elapsed = time.time() - t0\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} train_loss={np.mean(train_losses) if train_losses else float('nan'):.5f} train_mae={np.mean(train_maes) if train_maes else float('nan'):.5f} val_mae={avg_val_mae:.5f} time={elapsed:.1f}s\")\n",
        "        if avg_val_mae < best_val - 1e-6:\n",
        "            best_val = avg_val_mae\n",
        "            best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= PATIENCE:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "    # restore best weights\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    torch.save(model.state_dict(), os.path.join(MODEL_DIR, f\"tcn_fold_{fold_no}.pth\"))\n",
        "\n",
        "    # Evaluate on test files for this fold\n",
        "    X_test_parts = []; y_test_parts = []\n",
        "    for idx in test_files:\n",
        "        path = file_map.get(idx)\n",
        "        if path is None: continue\n",
        "        try:\n",
        "            raw = pd.read_csv(path, usecols=lambda c: c in FEATURE_COLS_BASE + TARGETS)\n",
        "        except Exception:\n",
        "            continue\n",
        "        if DOWNSAMPLE > 1:\n",
        "            raw = raw.iloc[::DOWNSAMPLE].reset_index(drop=True)\n",
        "        if len(raw) < SEQ_LEN + HORIZON:\n",
        "            continue\n",
        "        fdf = featurize_df(raw, cat_levels)\n",
        "        if fdf is None: continue\n",
        "        arr = fdf.values.astype('float32')\n",
        "        X_feats = scaler_X_global.transform(arr[:, :-2])\n",
        "        y_t = scaler_y_global.transform(arr[:, -2:])\n",
        "        arr_scaled = np.hstack([X_feats, y_t]).astype('float32')\n",
        "        Xw, yw = create_windows_from_scaled_array(arr_scaled, SEQ_LEN, HORIZON, MAX_SEQS_PER_FILE // 2)\n",
        "        if Xw is None: continue\n",
        "        X_test_parts.append(Xw); y_test_parts.append(yw)\n",
        "        del raw, fdf, arr, arr_scaled, X_feats, y_t, Xw, yw; gc.collect()\n",
        "    if len(X_test_parts) == 0:\n",
        "        print(\"No test windows for this fold -> continue\")\n",
        "        continue\n",
        "    X_test_all = np.vstack(X_test_parts); y_test_all = np.vstack(y_test_parts)\n",
        "    dl_test = DataLoader(TensorDataset(torch.from_numpy(X_test_all), torch.from_numpy(y_test_all)), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "    preds = []; trues = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in dl_test:\n",
        "            Xb = torch.nan_to_num(Xb, nan=0.0, posinf=0.0, neginf=0.0).to(DEVICE, dtype=torch.float)\n",
        "            if torch.isnan(Xb).any():\n",
        "                continue\n",
        "            with autocast():\n",
        "                out = model(Xb)\n",
        "            out_np = out.cpu().numpy(); y_np = yb.numpy()\n",
        "            out_inv = scaler_y_global.inverse_transform(out_np)\n",
        "            y_inv = scaler_y_global.inverse_transform(y_np)\n",
        "            preds.append(out_inv); trues.append(y_inv)\n",
        "    if len(preds) == 0:\n",
        "        print(\"No predictions for fold -> continue\")\n",
        "        continue\n",
        "    preds = np.vstack(preds); trues = np.vstack(trues)\n",
        "    pred_lens = preds[:,0]; pred_pw = preds[:,1]\n",
        "    true_lens = trues[:,0]; true_pw = trues[:,1]\n",
        "    global_pred_lens.extend(pred_lens.tolist()); global_true_lens.extend(true_lens.tolist())\n",
        "    global_pred_pw.extend(pred_pw.tolist()); global_true_pw.extend(true_pw.tolist())\n",
        "\n",
        "    metrics_lens = compute_metrics(true_lens, pred_lens)\n",
        "    metrics_pw = compute_metrics(true_pw, pred_pw)\n",
        "    all_fold_results.append({'fold': fold_no, 'metrics_lens': metrics_lens, 'metrics_pw': metrics_pw})\n",
        "    print(f\"Fold {fold_no} metrics (poll_lens): {metrics_lens}\")\n",
        "    print(f\"Fold {fold_no} metrics (poll_pw)  : {metrics_pw}\")\n",
        "\n",
        "    # cleanup\n",
        "    del model, optimizer, dl_tr, dl_val, dl_test, ds_tr, ds_val\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "r78EATXk9yzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# FINAL forced test on MANDATORY_TEST\n",
        "# -----------------------\n",
        "print(\"\\n=== FINAL forced test on MANDATORY_TEST:\", sorted(list(MANDATORY_TEST)))\n",
        "train_final = [i for i in usable if i not in MANDATORY_TEST]\n",
        "test_final = sorted(list(MANDATORY_TEST))\n",
        "\n",
        "cat_levels_final = collect_category_levels(train_final, max_rows_each=3000)\n",
        "\n",
        "# Fit final scalers on larger sample across train_final\n",
        "SAMPLE_ROWS_FINAL = SAMPLE_ROWS_FOR_SCALER\n",
        "X_parts = []; y_parts = []; rows = 0\n",
        "for idx in train_final:\n",
        "    p = file_map.get(idx)\n",
        "    if p is None: continue\n",
        "    try:\n",
        "        raw = pd.read_csv(p, usecols=lambda c: c in FEATURE_COLS_BASE + TARGETS, nrows=MAX_SAMPLES_PER_FILE_FOR_SCALER)\n",
        "    except Exception:\n",
        "        continue\n",
        "    if DOWNSAMPLE > 1:\n",
        "        raw = raw.iloc[::DOWNSAMPLE].reset_index(drop=True)\n",
        "    if len(raw) < 10: continue\n",
        "    fdf = featurize_df(raw, cat_levels_final)\n",
        "    if fdf is None: continue\n",
        "    vals = fdf.values\n",
        "    X_parts.append(vals[:, :-2]); y_parts.append(vals[:, -2:])\n",
        "    del raw, fdf, vals; gc.collect()\n",
        "    rows += 1\n",
        "    if rows >= len(train_final): break\n",
        "\n",
        "if len(X_parts) == 0:\n",
        "    raise RuntimeError(\"No sample rows for final scalers\")\n",
        "X_sample = np.vstack(X_parts); y_sample = np.vstack(y_parts)\n",
        "scaler_X_final = RobustScaler().fit(X_sample)\n",
        "scaler_y_final = StandardScaler().fit(y_sample)\n",
        "del X_parts, y_parts, X_sample, y_sample; gc.collect()\n",
        "\n",
        "# Build final train windows\n",
        "X_train_parts = []; y_train_parts = []\n",
        "for idx in train_final:\n",
        "    p = file_map.get(idx)\n",
        "    if p is None: continue\n",
        "    try:\n",
        "        raw = pd.read_csv(p, usecols=lambda c: c in FEATURE_COLS_BASE + TARGETS)\n",
        "    except Exception:\n",
        "        continue\n",
        "    if DOWNSAMPLE > 1:\n",
        "        raw = raw.iloc[::DOWNSAMPLE].reset_index(drop=True)\n",
        "    if len(raw) < SEQ_LEN + HORIZON: continue\n",
        "    fdf = featurize_df(raw, cat_levels_final)\n",
        "    if fdf is None: continue\n",
        "    arr = fdf.values.astype('float32')\n",
        "    X_feats = scaler_X_final.transform(arr[:, :-2])\n",
        "    y_t = scaler_y_final.transform(arr[:, -2:])\n",
        "    arr_scaled = np.hstack([X_feats, y_t]).astype('float32')\n",
        "    Xw, yw = create_windows_from_scaled_array(arr_scaled, SEQ_LEN, HORIZON, MAX_SEQS_PER_FILE)\n",
        "    if Xw is None: continue\n",
        "    X_train_parts.append(Xw); y_train_parts.append(yw)\n",
        "    del raw, fdf, arr, arr_scaled, X_feats, y_t, Xw, yw; gc.collect()\n",
        "if len(X_train_parts) == 0:\n",
        "    raise RuntimeError(\"No final train windows\")\n",
        "X_tr_all = np.vstack(X_train_parts); y_tr_all = np.vstack(y_train_parts)\n",
        "perm = np.random.permutation(len(X_tr_all)); X_tr_all = X_tr_all[perm]; y_tr_all = y_tr_all[perm]\n",
        "split = int(0.95 * len(X_tr_all))\n",
        "X_tr = X_tr_all[:split]; y_tr = y_tr_all[:split]\n",
        "X_val = X_tr_all[split:]; y_val = y_tr_all[split:]\n",
        "del X_tr_all, y_tr_all; gc.collect()\n",
        "\n",
        "# Build final test windows\n",
        "X_test_parts = []; y_test_parts = []\n",
        "for idx in test_final:\n",
        "    p = file_map.get(idx)\n",
        "    if p is None: continue\n",
        "    try:\n",
        "        raw = pd.read_csv(p, usecols=lambda c: c in FEATURE_COLS_BASE + TARGETS)\n",
        "    except Exception:\n",
        "        continue\n",
        "    if DOWNSAMPLE > 1:\n",
        "        raw = raw.iloc[::DOWNSAMPLE].reset_index(drop=True)\n",
        "    if len(raw) < SEQ_LEN + HORIZON: continue\n",
        "    fdf = featurize_df(raw, cat_levels_final)\n",
        "    if fdf is None: continue\n",
        "    arr = fdf.values.astype('float32')\n",
        "    X_feats = scaler_X_final.transform(arr[:, :-2])\n",
        "    y_t = scaler_y_final.transform(arr[:, -2:])\n",
        "    arr_scaled = np.hstack([X_feats, y_t]).astype('float32')\n",
        "    Xw, yw = create_windows_from_scaled_array(arr_scaled, SEQ_LEN, HORIZON, MAX_SEQS_PER_FILE//2)\n",
        "    if Xw is None: continue\n",
        "    X_test_parts.append(Xw); y_test_parts.append(yw)\n",
        "    del raw, fdf, arr, arr_scaled, X_feats, y_t, Xw, yw; gc.collect()\n",
        "if len(X_test_parts) == 0:\n",
        "    raise RuntimeError(\"No final test windows\")\n",
        "X_test_all = np.vstack(X_test_parts); y_test_all = np.vstack(y_test_parts)\n",
        "\n",
        "# Dataloaders final\n",
        "dl_tr = DataLoader(TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr)), batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "dl_val = DataLoader(TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val)), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "dl_test = DataLoader(TensorDataset(torch.from_numpy(X_test_all), torch.from_numpy(y_test_all)), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "# Train final model\n",
        "n_features_final = X_tr.shape[2]\n",
        "model_final = TCN(input_size=n_features_final, output_size=2, num_channels=[32,32], kernel_size=3, dropout=0.2).to(DEVICE)\n",
        "opt_final = torch.optim.AdamW(model_final.parameters(), lr=LR, weight_decay=1e-5)\n",
        "sch_final = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_final, mode='min', factor=0.5, patience=3)\n",
        "amp_final = GradScaler()\n",
        "\n",
        "best_val = 1e9; best_state = None; wait = 0\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    model_final.train()\n",
        "    losses = []; maes = []\n",
        "    for Xb, yb in dl_tr:\n",
        "        Xb = torch.nan_to_num(Xb, nan=0.0, posinf=0.0, neginf=0.0).to(DEVICE, dtype=torch.float)\n",
        "        yb = torch.nan_to_num(yb, nan=0.0, posinf=0.0, neginf=0.0).to(DEVICE, dtype=torch.float)\n",
        "        if torch.isnan(Xb).any() or torch.isnan(yb).any():\n",
        "            continue\n",
        "        opt_final.zero_grad()\n",
        "        with autocast():\n",
        "            out = model_final(Xb)\n",
        "            loss = torch.nn.SmoothL1Loss()(out, yb)\n",
        "        amp_final.scale(loss).backward()\n",
        "        amp_final.step(opt_final); amp_final.update()\n",
        "        losses.append(loss.item()); maes.append(torch.mean(torch.abs(out-yb)).item())\n",
        "    # val\n",
        "    model_final.eval()\n",
        "    val_losses=[]; val_maes=[]\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in dl_val:\n",
        "            Xb = torch.nan_to_num(Xb, nan=0.0, posinf=0.0, neginf=0.0).to(DEVICE, dtype=torch.float)\n",
        "            yb = torch.nan_to_num(yb, nan=0.0, posinf=0.0, neginf=0.0).to(DEVICE, dtype=torch.float)\n",
        "            if torch.isnan(Xb).any() or torch.isnan(yb).any():\n",
        "                continue\n",
        "            with autocast():\n",
        "                out = model_final(Xb)\n",
        "                loss = torch.nn.SmoothL1Loss()(out, yb)\n",
        "            val_losses.append(loss.item()); val_maes.append(torch.mean(torch.abs(out-yb)).item())\n",
        "    avg_val_mae = float(np.mean(val_maes)) if len(val_maes)>0 else float('inf')\n",
        "    val_loss_mean = float(np.mean(val_losses)) if len(val_losses)>0 else (np.mean(losses) if losses else 0.0)\n",
        "    sch_final.step(val_loss_mean)\n",
        "    print(f\"[FINAL] Epoch {epoch}/{EPOCHS} train_loss={np.mean(losses) if losses else float('nan'):.5f} val_mae={avg_val_mae:.5f} time={time.time()-t0:.1f}s\")\n",
        "    if avg_val_mae < best_val - 1e-6:\n",
        "        best_val = avg_val_mae; best_state = {k:v.cpu() for k,v in model_final.state_dict().items()}; wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= PATIENCE:\n",
        "            print(\"[FINAL] Early stopping\"); break\n",
        "\n",
        "if best_state is not None:\n",
        "    model_final.load_state_dict(best_state)\n",
        "torch.save(model_final.state_dict(), os.path.join(MODEL_DIR, \"tcn_final_optionB_cleaned.pth\"))\n",
        "\n",
        "# Final evaluation\n",
        "preds = []; trues = []\n",
        "model_final.eval()\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in dl_test:\n",
        "        Xb = torch.nan_to_num(Xb, nan=0.0, posinf=0.0, neginf=0.0).to(DEVICE, dtype=torch.float)\n",
        "        if torch.isnan(Xb).any():\n",
        "            continue\n",
        "        with autocast():\n",
        "            out = model_final(Xb)\n",
        "        out_np = out.cpu().numpy(); y_np = yb.numpy()\n",
        "        out_inv = scaler_y_final.inverse_transform(out_np)\n",
        "        y_inv = scaler_y_final.inverse_transform(y_np)\n",
        "        preds.append(out_inv); trues.append(y_inv)\n",
        "preds = np.vstack(preds); trues = np.vstack(trues)\n",
        "pred_lens_final = preds[:,0]; true_lens_final = trues[:,0]\n",
        "pred_pw_final = preds[:,1]; true_pw_final = trues[:,1]\n",
        "metrics_final_lens = compute_metrics(true_lens_final, pred_lens_final)\n",
        "metrics_final_pw = compute_metrics(true_pw_final, pred_pw_final)\n",
        "print(\"\\nFINAL TEST METRICS (poll_lens):\", metrics_final_lens)\n",
        "print(\"FINAL TEST METRICS (poll_pw)  :\", metrics_final_pw)\n",
        "\n",
        "# Aggregated folds\n",
        "agg_lens = compute_metrics(np.array(global_true_lens), np.array(global_pred_lens)) if len(global_true_lens)>0 else None\n",
        "agg_pw = compute_metrics(np.array(global_true_pw), np.array(global_pred_pw)) if len(global_true_pw)>0 else None\n",
        "print(\"\\nAGGREGATED FOLDS (lens):\", agg_lens)\n",
        "print(\"AGGREGATED FOLDS (pw)  :\", agg_pw)\n",
        "\n",
        "# Save results and scalers\n",
        "out = {\n",
        "    'config': {'DOWNSAMPLE': DOWNSAMPLE, 'SEQ_LEN': SEQ_LEN, 'HORIZON': HORIZON, 'usable_files': usable},\n",
        "    'folds': all_fold_results,\n",
        "    'final_test': {'lens': metrics_final_lens, 'pw': metrics_final_pw},\n",
        "    'aggregated': {'lens': agg_lens, 'pw': agg_pw}\n",
        "}\n",
        "joblib.dump(out, RESULTS_FILE)\n",
        "joblib.dump(scaler_X_global, os.path.join(MODEL_DIR, \"scaler_X_global.pkl\"))\n",
        "joblib.dump(scaler_y_global, os.path.join(MODEL_DIR, \"scaler_y_global.pkl\"))\n",
        "print(\"Saved results to\", RESULTS_FILE)\n",
        "print(\"Saved scalers to\", os.path.join(MODEL_DIR, \"scaler_*_global.pkl\"))\n"
      ],
      "metadata": {
        "id": "ayvfwyr7-SSp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
